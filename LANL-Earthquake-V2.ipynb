{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LANL Earthquake Prediction Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/LANL-Title-Image.jpg\" align=left width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This project aims at making time untill earthquake predictions based off of simulated seismic signal data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data analysis tools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# plotting\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# project tools\n",
    "import LANL_Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Load Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 37s, sys: 24 s, total: 3min 1s\n",
      "Wall time: 3min 6s\n"
     ]
    }
   ],
   "source": [
    "# reset directory and load data\n",
    "import os\n",
    "os.chdir(\"/\")\n",
    "%time df_train = pd.read_csv('Users/gregeales/Desktop/LANL-Data/train.csv', dtype = {'acoustic_data': np.int16, 'time_to_failure': np.float32}) # float32 is enough :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Statistical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Acoustic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    6.2914548e+08\n",
       "mean     4.5194676e+00\n",
       "std      1.0735707e+01\n",
       "min     -5.5150000e+03\n",
       "25%      2.0000000e+00\n",
       "50%      5.0000000e+00\n",
       "75%      7.0000000e+00\n",
       "max      5.4440000e+03\n",
       "Name: acoustic_data, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option(\"display.precision\", 7)\n",
    "df_train.acoustic_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = df_train.sample(frac=0.001)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Acoustic data distribution\")\n",
    "ax = plt.plot(train_sample.acoustic_data, label='Train (1% sample)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Time to Failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.precision\", 7)\n",
    "df_train.time_to_failure.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Feature Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import LANL Tools\n",
    "from LANL_Tools.feature_generator import FeatureGenerator\n",
    "from LANL_Tools.feature_functions import *\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 Generate Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_fg = FeatureGenerator(dtype='train', n_jobs=20, chunk_size=150000)\n",
    "training_data = training_fg.generate()\n",
    "\n",
    "test_fg = FeatureGenerator(dtype='test', n_jobs=20, chunk_size=150000)\n",
    "test_data = test_fg.generate()\n",
    "\n",
    "X = training_data.drop(['target', 'seg_id'], axis=1)\n",
    "X_test = test_data.drop(['target', 'seg_id'], axis=1)\n",
    "test_segs = test_data.seg_id\n",
    "y = training_data.target\n",
    "\n",
    "means_dict = {}\n",
    "for col in X.columns:\n",
    "    if X[col].isnull().any():\n",
    "        print(col)\n",
    "        mean_value = X.loc[X[col] != -np.inf, col].mean()\n",
    "        X.loc[X[col] == -np.inf, col] = mean_value\n",
    "        X[col] = X[col].fillna(mean_value)\n",
    "        means_dict[col] = mean_value\n",
    "        \n",
    "for col in X_test.columns:\n",
    "    if X_test[col].isnull().any():\n",
    "        X_test.loc[X_test[col] == -np.inf, col] = means_dict[col]\n",
    "        X_test[col] = X_test[col].fillna(means_dict[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3 Define Basic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold = 5\n",
    "folds = KFold(n_splits=n_fold, shuffle=True, random_state=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.4 Train Basic Model and Display Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters\n",
    "params = {'num_leaves': 128, 'min_data_in_leaf': 79, 'objective': 'gamma', 'max_depth': -1, 'learning_rate': 0.01, \n",
    "          \"boosting\": \"gbdt\", \"bagging_freq\": 5, \"bagging_fraction\": 0.8126672064208567, \"bagging_seed\": 11,\n",
    "          \"metric\": 'mae', \"verbosity\": -1, 'reg_alpha': 0.1302650970728192, 'reg_lambda': 0.3603427518866501,\n",
    "          'feature_fraction': 0.2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train basic model\n",
    "oof_lgb, prediction_lgb, feature_importance = train_model(X, X_test, y, params=params, model_type='lgb',\n",
    "                                                          plot_feature_importance=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.5 Graph Prediction Output vs Actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 8))\n",
    "plt.plot(y, color='g', label='y_train')\n",
    "plt.plot(oof_lgb, color='b', label='lgb')\n",
    "plt.legend(loc=(1, 0.5));\n",
    "plt.title('lgb');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.6 Save Feature Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset directory and load data\n",
    "import os\n",
    "os.chdir(\"/\")\n",
    "X.to_csv('Users/gregeales/Desktop/LANL-Data/train_features.csv', index=False)\n",
    "X_test.to_csv('Users/gregeales/Desktop/LANL-Data/test_features.csv', index=False)\n",
    "pd.DataFrame(y).to_csv('Users/gregeales/Desktop/LANL-Data/y.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import LANL Tools\n",
    "from LANL_Tools.feature_generator import FeatureGenerator\n",
    "from LANL_Tools.feature_functions import *\n",
    "\n",
    "# import libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Load Featurized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset directory and load feature data\n",
    "import os\n",
    "os.chdir(\"/\")\n",
    "path = \"Users/gregeales/Desktop/LANL-Data/\"\n",
    "train_features = pd.read_csv(path + 'train_features.csv')\n",
    "test_features = pd.read_csv(path + 'test_features.csv')\n",
    "train_features_denoised = pd.read_csv(path + 'train_features_denoised.csv')\n",
    "test_features_denoised = pd.read_csv(path + 'test_features_denoised.csv')\n",
    "train_features_denoised.columns = [f'{i}_denoised' for i in train_features_denoised.columns]\n",
    "test_features_denoised.columns = [f'{i}_denoised' for i in test_features_denoised.columns]\n",
    "y = pd.read_csv(path + 'y.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add denoised and un-denoised data together in case of information loss\n",
    "X = pd.concat([train_features, train_features_denoised], axis=1).drop(['seg_id_denoised', 'target_denoised'], axis=1)\n",
    "X_test = pd.concat([test_features, test_features_denoised], axis=1).drop(['seg_id_denoised', 'target_denoised'], axis=1)\n",
    "# remove last line due to it being less than regular interval\n",
    "X = X[:-1]\n",
    "y = y[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Scale Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate scaled data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_train_scaled = pd.DataFrame(scaler.transform(X), columns=X.columns)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "X_train_scaled_holder = pd.DataFrame(scaler.transform(X), columns=X.columns)\n",
    "X_test_scaled_holder = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 15s, sys: 507 ms, total: 2min 15s\n",
      "Wall time: 38.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n = 10\n",
    "neigh = NearestNeighbors(n, n_jobs=-1)\n",
    "neigh.fit(X_train_scaled)\n",
    "\n",
    "dists, _ = neigh.kneighbors(X_train_scaled, n_neighbors=n)\n",
    "mean_dist = dists.mean(axis=1)\n",
    "max_dist = dists.max(axis=1)\n",
    "min_dist = dists.min(axis=1)\n",
    "\n",
    "X_train_scaled['mean_dist'] = mean_dist\n",
    "X_train_scaled['max_dist'] = max_dist\n",
    "X_train_scaled['min_dist'] = min_dist\n",
    "\n",
    "test_dists, _ = neigh.kneighbors(X_test_scaled, n_neighbors=n)\n",
    "\n",
    "test_mean_dist = test_dists.mean(axis=1)\n",
    "test_max_dist = test_dists.max(axis=1)\n",
    "test_min_dist = test_dists.min(axis=1)\n",
    "\n",
    "X_test_scaled['mean_dist'] = test_mean_dist\n",
    "X_test_scaled['max_dist'] = test_max_dist\n",
    "X_test_scaled['min_dist'] = test_min_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Save Normalized Feature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled.to_csv('Users/gregeales/Desktop/LANL-Data/X_test_scaled.csv', index=False)\n",
    "X_train_scaled.to_csv('Users/gregeales/Desktop/LANL-Data/X_train_scaled.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Load Time Series Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load time series data for reccurent networks\n",
    "import os\n",
    "os.chdir(\"/\")\n",
    "#float_data = np.load(\"kaggle/input/lanl-lstm-data/LSTM_Training_Data.npy\") \n",
    "batch_size = 32\n",
    "n_features=12\n",
    "second_earthquake = 50085877"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 Load Normalized Feature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset directory and load nomralized feature data\n",
    "import os\n",
    "os.chdir(\"/\")\n",
    "path = \"Users/gregeales/Desktop/LANL-Data/\"\n",
    "X_train_scaled = pd.read_csv(path + 'X_train_scaled.csv')\n",
    "X_test_scaled = pd.read_csv(path + 'X_test_scaled.csv')\n",
    "y = pd.read_csv(path + 'y.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Reccurent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, RNN\n",
    "from keras.optimizers import adam\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Define and Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('`cell` should have a `call` method. The RNN was passed:', 65)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-5fb4be6d49d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model.hdf5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m65\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m75\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cell, return_sequences, return_state, go_backwards, stateful, unroll, **kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m             raise ValueError('`cell` should have a `call` method. '\n\u001b[0;32m--> 402\u001b[0;31m                              'The RNN was passed:', cell)\n\u001b[0m\u001b[1;32m    403\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'state_size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m             raise ValueError('The RNN cell should have '\n",
      "\u001b[0;31mValueError\u001b[0m: ('`cell` should have a `call` method. The RNN was passed:', 65)"
     ]
    }
   ],
   "source": [
    "cb = [ModelCheckpoint(\"model.hdf5\", save_best_only=True, period=1, verbose=1, monitor='val_loss')]\n",
    "model = Sequential()\n",
    "model.add(RNN(65, return_sequences=True,input_shape=(None, n_features)))\n",
    "model.add(RNN(50))\n",
    "model.add(Dense(75, activation='relu'))\n",
    "model.add(Dense(40, activation='relu'))\n",
    "model.add(Dense(1, activation='relu'))\n",
    "model.summary()\n",
    "model.compile(optimizer=adam(lr=0.0005), loss=\"mae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(train_gen, steps_per_epoch=1000, epochs=40, verbose=0, validation_data=valid_gen,\n",
    "                                              validation_steps=200, callbacks=cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 LGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1 Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.1 Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.1 Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Refrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
